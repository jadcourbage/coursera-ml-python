{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Machine Learning Online Class \n",
    "##  Exercise 6 - Part 2 | Spam Classification with SVMs\n",
    "Requires : nltk, scipy, scikit-learn <br />\n",
    "For nltk <br />\n",
    "pip install nltk <br />\n",
    "then in a python console : <br />\n",
    "import nltk <br />\n",
    "nltk.download() <br />\n",
    "Choose all packages"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Introduction"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Many email services today provide spam filters that are able to classify emails into spam and non-spam email with high accuracy.<br><br>\n",
    "In this part of the exercise, we will use SVMs to build our own spam filter.<br>\n",
    "We will be training a classifier to classify whether a given email, x, is spam ($y = 1$) or non-spam ($y = 0$). <br>\n",
    "In particular, you need to convert each email into a feature vector $x \\in \\mathbb{R}^n$. <br>\n",
    "The following parts of the exercise will walk through how such a feature vector can be constructed from an email.<br><br>\n",
    "The dataset included for this exercise is based on a a subset of the SpamAssassin Public Corpus.<br>\n",
    "For the purpose of this exercise, we will only be using the body of the email (excluding the email headers)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Python Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "import string\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from scipy.io import loadmat\n",
    "from sklearn.svm import LinearSVC \n",
    "from sklearn.metrics import accuracy_score\n",
    "import re               # regexp \n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.stem.porter import *\n",
    "from IPython.display import Image"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Part 1: Email processing "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "file_contents = open('emailSample1.txt', 'r').read()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Before starting on a machine learning task, it is usually insightful to take a look at examples from the dataset. <br>\n",
    "Figure below shows a sample email that contains a URL, an email address (at the end), numbers, and dollar\n",
    "amounts. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![title](SampleEmail.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "While many emails would contain similar types of entities (e.g. numbers, other URLs, or other email addresses), the specific entities (e.g., the specific URL or specific dollar amount) will be different in almost every email. <br><br>\n",
    "Therefore, one method often employed in processing emails is to \"normalize\" these values, so that all URLs are treated the same, all numbers are treated the same, etc. <br>\n",
    "For example, we could replace each URL in the email with the unique string \"httpaddr\" to indicate that a URL was present.\n",
    "This has the effect of letting the spam classifier make a classification decision based on whether any URL was present, rather than whether a specific URL was present. <br><br>\n",
    "This typically improves the performance of a spam classifier, since spammers often randomize the URLs, and thus the odds of seeing any particular URL again in a new piece of spam is very small."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def preProcessEmail(email_contents):\n",
    "    \n",
    "    #Preprocess email\n",
    "    \n",
    "    # Lower case\n",
    "    email_contents = email_contents.lower()\n",
    "    \n",
    "    # Strip all HTML\n",
    "    # Looks for any expression that starts with < and ends with > and replace\n",
    "    # and does not have any < or > in the tag it with a space    \n",
    "    email_contents = re.sub('<[^<>]+>',' ', email_contents)\n",
    "    \n",
    "    # Handle numbers\n",
    "    # Look for one or more characters between 0-9 and replace by 'number'\n",
    "    email_contents = re.sub('[0-9]+','number', email_contents)\n",
    "    \n",
    "    # Handle URLS\n",
    "    # Look for strings starting with http:// or https:// and replace by httpaddr\n",
    "    email_contents = re.sub('(http|https)://[^\\s]*','httpaddr', email_contents)\n",
    "    \n",
    "    # Handle Email Addresses\n",
    "    # Look for strings with @ in the middle and replace by emailaddr\n",
    "    email_contents = re.sub('[^\\s]+@[^\\s]+','emailaddr', email_contents)\n",
    "    \n",
    "    # Handle $ sign\n",
    "    email_contents = re.sub('[$]+','dollar', email_contents)\n",
    "   \n",
    "    # Get rid of special characters\n",
    "    email_contents = re.sub('[^a-zA-Z0-9]',' ', email_contents)\n",
    "    \n",
    "    return email_contents    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "email_preprocessed = preProcessEmail(file_contents)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Then we tokenize emails, ie ie split the text into a list of words and stem the words using Porter stemmer\n",
    "(https://en.wikipedia.org/wiki/Stemming)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def tokenizeEmail(email_contents):\n",
    "    # Tokenize email, ie split the text into a list of words\n",
    "    # And stem the words (https://en.wikipedia.org/wiki/Stemming)\n",
    "    \n",
    "    # NLTK word_tokenize method\n",
    "    tokenized_email = word_tokenize(email_contents)\n",
    "\n",
    "    # Stem the words using Porter stemmer\n",
    "    stemmer = PorterStemmer()\n",
    "    return [stemmer.stem(word) for word in tokenized_email]\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "email_tokenized = tokenizeEmail(email_preprocessed)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After preprocessing the emails, we have a list of words for each email, for example :<br><br>\n",
    "![title](PreprocessedSample.png)\n",
    "<br>\n",
    "The next step is to choose which words we would like to use in our classifier and which we would want to leave out.<br>\n",
    "For this exercise, we have chosen only the most frequently occuring words as our set of words considered (the vocabulary list).<br> Since words that occur rarely in the training set are only in a few emails, they might cause the model to overfit our training set.\n",
    "The complete vocabulary list is in the file <i>vocab.txt</i>\n",
    "![title](VocabList.png)\n",
    "<br>\n",
    "Our vocabulary list was selected by choosing all words which occur at least a 100 times in the spam corpus, resulting in a list of 1899 words. <br>\n",
    "In practice, a vocabulary list with about 10,000 to 50,000 words is often used.<br>\n",
    "Given the vocabulary list, we can now map each word in the preprocessed emails into a list of word indices that contains the index of the word in the vocabulary list. <br>\n",
    "![title](WordIndices.png)\n",
    "<br>\n",
    "Specifically, in the sample email, the word \"anyone\" was first normalized to \"anyon\" and then mapped onto the index 86 in the vocabulary list.<br>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def indexEmail(email_tokenized):\n",
    "       \n",
    "    #Load vocabulary\n",
    "    vocabList = pd.read_csv('vocab.txt', delimiter = '\\t' , header = None).values\n",
    "    \n",
    "    #Return indices of words contained in vocabulary\n",
    "    return [np.asscalar(np.argwhere(vocabList[:,1] == w)) for w in email_tokenized if np.argwhere(vocabList[:,1] == w).size > 0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "word_indices = indexEmail(email_tokenized)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Part 2: Feature Extraction"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will now implement the feature extraction that converts each email into a vector in \\mathbb{R}^N.\n",
    "For this exercise, we will be using n = # words in vocabulary list. \n",
    "Specifically, the feature $x_t \\in \\{0,1\\}$ for an email corresponds to whether the $i$ -th word in the dictionary occurs in the email. That is, $x_i = 1$ if the $i$-th word is in the email and $x_i = 1$  $i$-th word is not present in the email.\n",
    "\n",
    "Thus, for a typical email, this feature would look like:\n",
    "$$ x=\n",
    "\\quad\n",
    "\\begin{bmatrix} \n",
    "0 \\\\\n",
    "\\vdots \\\\\n",
    "\\vdots  \\\\\n",
    "1 \\\\\n",
    "0 \\\\\n",
    "\\vdots\\\\\n",
    "1\n",
    "\\end{bmatrix}\n",
    "\\in \\mathbb{R}^n\n",
    "$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def emailFeatures(word_indices):\n",
    "    \n",
    "    #takes in a word_indices vector and produces a feature vector from the word indices\n",
    "    \n",
    "    n = 1899 #Total number of words in the dictionary\n",
    "    \n",
    "    word_indices = np.array(word_indices)\n",
    "    \n",
    "    feat = np.zeros(n)\n",
    "    \n",
    "    for i in range(word_indices.size):\n",
    "        feat[word_indices[i]] = 1\n",
    "    \n",
    "    return feat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "features = emailFeatures(word_indices)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Length of feature vector : 1899\n",
      "Number of non-zero entries : 45\n"
     ]
    }
   ],
   "source": [
    "print('Length of feature vector : {:d}'.format(features.size))\n",
    "print('Number of non-zero entries : {:d}'.format(np.sum(features > 0)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Part 3: Train Linear SVM for Spam Classification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "data = loadmat('spamTrain.mat')\n",
    "X = data['X'] # X is already preformatted with feature vectors of 0 and 1\n",
    "y = data['y'].ravel()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# declare linear SVM model\n",
    "model = LinearSVC(tol = 1e-3, C = 0.1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "LinearSVC(C=0.1, class_weight=None, dual=True, fit_intercept=True,\n",
       "     intercept_scaling=1, loss='squared_hinge', max_iter=1000,\n",
       "     multi_class='ovr', penalty='l2', random_state=None, tol=0.001,\n",
       "     verbose=0)"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# fit on training data\n",
    "model.fit(X,y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training accuracy : 99.98 %\n"
     ]
    }
   ],
   "source": [
    "print('Training accuracy : {:2.2f} %'.format(accuracy_score(y,model.predict(X))*100))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Part 4: Test Spam Classification\n",
    "After training the classifier, we can evaluate it on a test set. We have\n",
    "included a test set in spamTest.mat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "data_test = loadmat('spamTest.mat')\n",
    "Xtest = data_test['Xtest']\n",
    "ytest = data_test['ytest'].ravel()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test accuracy : 99.20 %\n"
     ]
    }
   ],
   "source": [
    "print('Test accuracy : {:2.2f} %'.format(accuracy_score(ytest,model.predict(Xtest))*100))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Part 5: Top Predictors of Spam\n",
    "Since the model we are training is a linear SVM, we can inspect the\n",
    "weights learned by the model to understand better how it is determining\n",
    "whether an email is spam or not. The following code finds the words with\n",
    "the highest weights in the classifier. Informally, the classifier\n",
    "'thinks' that these words are the most likely indicators of spam.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Sort weights and store associated indices\n",
    "weights = np.sort(model.coef_[0,:], axis = 0)\n",
    "idx = np.argsort(model.coef_[0,:], axis = 0)\n",
    "\n",
    "# Reverse order (so that it is in descending order)\n",
    "weights = weights[::-1]\n",
    "idx = idx[::-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Retrieve vocabulary list\n",
    "vocabList = pd.read_csv('vocab.txt', delimiter = '\\t' , header = None).values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Top 1 predictor : our (-0.016812)\n",
      "Top 2 predictor : remov (-0.037361)\n",
      "Top 3 predictor : click (0.060263)\n",
      "Top 4 predictor : basenumb (0.093576)\n",
      "Top 5 predictor : guarante (0.009091)\n",
      "Top 6 predictor : visit (-0.115602)\n",
      "Top 7 predictor : bodi (0.083997)\n",
      "Top 8 predictor : will (-0.154599)\n",
      "Top 9 predictor : numberb (-0.012502)\n",
      "Top 10 predictor : price (-0.026996)\n",
      "Top 11 predictor : dollar (0.034485)\n",
      "Top 12 predictor : nbsp (-0.009752)\n",
      "Top 13 predictor : below (0.089140)\n",
      "Top 14 predictor : lo (-0.003161)\n",
      "Top 15 predictor : most (-0.008167)\n"
     ]
    }
   ],
   "source": [
    "# Top 15 predictors of spam\n",
    "for i in range(15):\n",
    "    print('Top {:d} predictor : {} ({:2.6f})'.format(i+1,vocabList[idx[i],1],weights[idx[i]]))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Part 6: Try Your Own Emails"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Do You Want To Make $1000 Or More Per Week?\n",
      "\n",
      " \n",
      "\n",
      "If you are a motivated and qualified individual - I \n",
      "will personally demonstrate to you a system that will \n",
      "make you $1,000 per week or more! This is NOT mlm.\n",
      "\n",
      " \n",
      "\n",
      "Call our 24 hour pre-recorded number to get the \n",
      "details.  \n",
      "\n",
      " \n",
      "\n",
      "000-456-789\n",
      "\n",
      " \n",
      "\n",
      "I need people who want to make serious money.  Make \n",
      "the call and get the facts. \n",
      "\n",
      "Invest 2 minutes in yourself now!\n",
      "\n",
      " \n",
      "\n",
      "000-456-789\n",
      "\n",
      " \n",
      "\n",
      "Looking forward to your call and I will introduce you \n",
      "to people like yourself who\n",
      "are currently making $10,000 plus per week!\n",
      "\n",
      " \n",
      "\n",
      "000-456-789\n",
      "\n",
      "\n",
      "\n",
      "3484lJGv6-241lEaN9080lRmS6-271WxHo7524qiyT5-438rjUv5615hQcf0-662eiDB9057dMtVl72\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Read email\n",
    "file_contents = open('spamSample1.txt', 'r').read()\n",
    "print(file_contents)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Spam Classification : 1\n",
      "(1 indicates spam, 0 indicates not spam)\n"
     ]
    }
   ],
   "source": [
    "# Transform to a feature vector\n",
    "email_preprocessed = preProcessEmail(file_contents)\n",
    "email_tokenized = tokenizeEmail(email_preprocessed)\n",
    "word_indices = indexEmail(email_tokenized)\n",
    "\n",
    "x = emailFeatures(word_indices)\n",
    "\n",
    "# Predict using previously trained model\n",
    "pred = model.predict(x.reshape(1,-1))[0]\n",
    "\n",
    "print('Spam Classification : {:d}'.format(pred))\n",
    "print('(1 indicates spam, 0 indicates not spam)')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## END"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
